{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid19-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhorapb/covid19-pytorch/blob/master/covid19_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uog5XjZ-5ZSk",
        "colab_type": "code",
        "outputId": "654343f3-c5be-4a13-d4af-5e1e25055eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46zJ_U9zbgtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Model for transfer learning from CheXNet by: \n",
        "- Pretraining (Feature Extraction): training only \n",
        "the output layer (last fully-connected one).\n",
        "We are using here the \"freezing\" approach.\n",
        "- Fine Tuning: updating all the weights of the model.\n",
        "\"\"\"\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import (\n",
        "    datasets, \n",
        "    models, \n",
        "    transforms, \n",
        "    utils\n",
        ")\n",
        "\n",
        "# Image imports\n",
        "# from skimage import io, transform\n",
        "# from PIL import Image\n",
        "\n",
        "# General imports\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from shutil import copyfile\n",
        "from shutil import rmtree\n",
        "from pathlib import Path\n",
        "\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# import covid_dataset as COVID_XR\n",
        "# import eval_model as E\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MaaDAgV6Uvy",
        "colab_type": "code",
        "outputId": "eab933f6-29b6-49c1-e61f-b99ab024c05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "root_path = '/drive/My Drive/1-COVID-19_DeepLearning/'\n",
        "# os.chdir(root_path)\n",
        "%cd drive/My\\ Drive/1-COVID-19_DeepLearning/\n",
        "!ls\n",
        "# !git clone https://github.com/jhorapb/covid19-pytorch.git\n",
        "# print(os.listdir())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/1-COVID-19_DeepLearning\n",
            "covid19-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "splS-Rvmru-g",
        "colab_type": "code",
        "outputId": "58ce53ac-fff3-44b2-ea60-ae5c04ad3191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd covid19-pytorch/covid_models/\n",
        "!git pull"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/1-COVID-19_DeepLearning/covid19-pytorch/covid_models\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqO0A1A09OKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "RESULTS_PATH = '../results/'\n",
        "use_gpu = torch.cuda.is_available()\n",
        "# device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "# print(\"Device: \" + str(device))\n",
        "gpu_count = torch.cuda.device_count()\n",
        "print(\"Available GPU count: \" + str(gpu_count))\n",
        "\n",
        "\n",
        "def load_checkpoint(PATH_CHECKPOINT, mode='state_dict', fine_tuning=False):\n",
        "    #####\n",
        "    # The pre-trained model checkpoint from 'reproduce-chexnet' contains:\n",
        "    # state = {\n",
        "    #     'model': model,\n",
        "    #     'best_loss': best_loss,\n",
        "    #     'epoch': epoch,\n",
        "    #     'rng_state': torch.get_rng_state(),\n",
        "    #     'LR': LR\n",
        "    # }\n",
        "    #####\n",
        "    \n",
        "    # Define new base model\n",
        "    model_tl = models.densenet121(pretrained=False)\n",
        "    model_dict = model_tl.state_dict()\n",
        "    \n",
        "    # Locate checkpoint\n",
        "    chexnet_checkpoint = torch.load(PATH_CHECKPOINT, map_location=torch.device('cpu'))\n",
        "    if mode == 'state_dict':\n",
        "        # Load pretrained CheXNet model (mode state_dict)\n",
        "        state_dict_chexnet = chexnet_checkpoint['state_dict']\n",
        "        # model_tl = torch.nn.DataParallel(model_tl)\n",
        "    else:\n",
        "        # Load pretrained CheXNet model (mode full_model)\n",
        "        chexnet_model = chexnet_checkpoint['model']\n",
        "        state_dict_chexnet = chexnet_model.state_dict()\n",
        "    \n",
        "    # 1. Filter out unnecessary keys\n",
        "    state_dict_chexnet = {k: v for k, v in state_dict_chexnet.items() \n",
        "                          if k in model_dict}\n",
        "    # 2. Overwrite entries in the existing state dict\n",
        "    # model_tl.update(state_dict_chexnet)\n",
        "    # 3. Load the new state dict\n",
        "    model_tl.load_state_dict(model_dict)    \n",
        "    \n",
        "    # epoch = chexnet_checkpoint['epoch']\n",
        "    # loss = chexnet_checkpoint['loss']\n",
        "    # LR = chexnet_checkpoint['LR']\n",
        "    \n",
        "    if not fine_tuning:\n",
        "      # Freeze the parameters for feature extraction (ony for pretraining), \n",
        "      # NO Fine Tuning\n",
        "      for parameter in model_tl.parameters():\n",
        "          parameter.requires_grad = False\n",
        "    \n",
        "    del chexnet_checkpoint\n",
        "    return model_tl\n",
        "\n",
        "\n",
        "def save_checkpoint(model, best_loss, epoch, LR, optimizer):\n",
        "    \"\"\"\n",
        "    Saves checkpoint of torchvision model during training.\n",
        "\n",
        "    Args:\n",
        "        model: torchvision model to be saved\n",
        "        best_loss: best val loss achieved so far in training\n",
        "        epoch: current epoch of training\n",
        "        LR: current learning rate in training\n",
        "        optimizer: pytorch optimizer to be saved\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    state = {\n",
        "        'model': model.state_dict(),\n",
        "        'best_loss': best_loss,\n",
        "        'epoch': epoch,\n",
        "        'rng_state': torch.get_rng_state(),\n",
        "        'LR': LR,\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    torch.save(state, RESULTS_PATH + 'tl_pretraining_checkpoint')\n",
        "\n",
        "\n",
        "def train_model(\n",
        "        model,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        LR,\n",
        "        num_epochs,\n",
        "        dataloaders,\n",
        "        dataset_sizes,\n",
        "        weight_decay):\n",
        "    \"\"\"\n",
        "    Fine tunes torchvision model to COVID-19 CXR data.\n",
        "\n",
        "    Args:\n",
        "        model: torchvision model to be finetuned (densenet-121 in this case)\n",
        "        criterion: loss criterion (binary cross entropy loss, BCELoss)\n",
        "        optimizer: optimizer to use in training (SGD)\n",
        "        LR: learning rate\n",
        "        num_epochs: continue training up to this many epochs\n",
        "        dataloaders: pytorch train and val dataloaders\n",
        "        dataset_sizes: length of train and val datasets\n",
        "        weight_decay: weight decay parameter we use in SGD with momentum\n",
        "    Returns:\n",
        "        model: trained torchvision model\n",
        "        best_epoch: epoch on which best model val loss was obtained\n",
        "\n",
        "    \"\"\"\n",
        "    since = time.time()\n",
        "\n",
        "    start_epoch = 1\n",
        "    best_loss = 999999\n",
        "    best_acc = 0.0\n",
        "    best_epoch = -1\n",
        "    last_train_loss = -1\n",
        "\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 17)\n",
        "\n",
        "        # set model to train or eval mode based on whether we are in train or\n",
        "        # val; necessary to get correct predictions given batchnorm\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                # model.train(True)\n",
        "                model.train(True)\n",
        "            else:\n",
        "                # model.train(False)\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "            total_done = 0\n",
        "            \n",
        "            # Iterate over dataset (train/val)\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                batch_size = inputs.shape[0]\n",
        "                inputs = Variable(inputs.cuda())\n",
        "                labels = Variable(labels.cuda()) #.long()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):    \n",
        "                    # Compute loss\n",
        "                    outputs = model(inputs)\n",
        "                    something_else, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                \n",
        "                # Backward pass: compute gradient and update \n",
        "                # parameters in training \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Computing loss for the whole batch dataset\n",
        "                running_loss += loss.item() * batch_size\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            \n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                last_train_loss = epoch_loss\n",
        "\n",
        "            print('{} epoch {}=> Loss: {:.4f} | Acc: {:.4f} | Data size: {}'.format(\n",
        "                phase, epoch, epoch_loss, epoch_acc, dataset_sizes[phase]))\n",
        "\n",
        "            if phase == 'val':\n",
        "                # # Decay learning rate if validation loss plateaus in this epoch\n",
        "                # if epoch_loss > best_loss:\n",
        "                #     decayed_LR = LR / 10\n",
        "                #     print('Decay Loss from {} to {} \\\n",
        "                #             as not seeing improvement in val loss'.format(\n",
        "                #                 str(LR), str(decayed_LR))\n",
        "                #             )\n",
        "                    # LR = decayed_LR\n",
        "                    # # Create new optimizer with lower learning rate\n",
        "                    # optimizer = optim.Adam(\n",
        "                    #     filter(\n",
        "                    #         lambda p: p.requires_grad, \n",
        "                    #         model_tl.parameters()), \n",
        "                    #     lr=LR, betas=(0.9, 0.999))\n",
        "                #     print(\"Created new optimizer with LR \" + str(LR))\n",
        "                \n",
        "                # Checkpoint model if has best val loss yet\n",
        "                if epoch_loss < best_loss:\n",
        "                    best_loss = epoch_loss\n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_epoch = epoch\n",
        "                    save_checkpoint(model, best_loss, epoch, LR, optimizer)\n",
        "\n",
        "                # Log training and validation loss over each epoch\n",
        "                with open(RESULTS_PATH + '/log_train', 'a') as logfile:\n",
        "                    logwriter = csv.writer(logfile, delimiter=',')\n",
        "                    if(epoch == 1):\n",
        "                        logwriter.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
        "                    logwriter.writerow([epoch, last_train_loss, epoch_loss])\n",
        "\n",
        "        total_done += batch_size\n",
        "        if(total_done % (100 * batch_size) == 0):\n",
        "            print(\"completed \" + str(total_done) + \" so far in epoch\")\n",
        "\n",
        "        # Apply early stopping if there is no val loss improvement in 3 epochs\n",
        "        if ((epoch - best_epoch) >= 8):\n",
        "            print(\"No improvement in the model accuracy in 3 epochs, stop!\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best Validation Loss: {:4f}'.format(best_loss))\n",
        "    print('Best Validation Accuracy: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights to return\n",
        "    checkpoint_best = torch.load(RESULTS_PATH + 'tl_pretraining_checkpoint')\n",
        "    model = checkpoint_best['model']\n",
        "\n",
        "    return model, best_epoch\n",
        "\n",
        "\n",
        "def perform_tl_cnn(PATH_TO_IMAGES, CHEXNET_CHECKPOINT, checkpoint_type, \n",
        "                   LR, WEIGHT_DECAY, FINE_TUNING):\n",
        "    \"\"\"\n",
        "    Trains model to COVID-19 dataset.\n",
        "\n",
        "    Args:\n",
        "        PATH_TO_IMAGES: path to COVID-19 image data collection\n",
        "        LR: learning rate\n",
        "        WEIGHT_DECAY: weight decay parameter for SGD\n",
        "\n",
        "    Returns:\n",
        "        preds: torchvision model predictions on test fold with ground truth for comparison\n",
        "        aucs: AUCs for each train,test tuple\n",
        "    \"\"\"\n",
        "    NUM_EPOCHS = 12 # 10 # 20\n",
        "    # Since the COVID-19 dataset at the moment is considerably small, \n",
        "    # it makes sense to use Batch Gradient Descent (all the samples \n",
        "    # being used to update the model parameters)\n",
        "    minibatch_gd = False\n",
        "    BATCH_SIZE = 375 if not minibatch_gd else 10\n",
        "\n",
        "    # Create path to save model results\n",
        "    Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ImageNet parameters for normalization\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # Binary classifier\n",
        "    # N_LABELS = 1\n",
        "    # Multi-class classifier\n",
        "    N_LABELS = 2\n",
        "\n",
        "    # load labels\n",
        "    # df = pd.read_csv(\"covid19_labels.csv\", index_col=0)\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(means, stds)\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(means, stds)\n",
        "        ]),\n",
        "    }\n",
        "    \n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(PATH_TO_IMAGES, x), \n",
        "                                              data_transforms[x]) \n",
        "                      for x in ['train', 'val']}\n",
        "    \n",
        "    # Option num. workers 8\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, \n",
        "                                                  shuffle=True, num_workers=1) \n",
        "                   for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "    print('CLASS NAMES\\n:', class_names, '\\n')\n",
        "    \n",
        "    # Load pre-trained CheXNet model\n",
        "    model_tl = load_checkpoint(CHEXNET_CHECKPOINT, mode=checkpoint_type, \n",
        "                               fine_tuning=FINE_TUNING)\n",
        "\n",
        "    # Verify if GPU is available\n",
        "    if not use_gpu:\n",
        "        raise ValueError(\"GPU is required\")\n",
        "    if use_gpu:\n",
        "        model_tl = model_tl.cuda()\n",
        "\n",
        "    # print('Pre-trained Model:\\n', model_tl)\n",
        "    num_ftrs = model_tl.classifier.in_features\n",
        "    # Size of each output sample.\n",
        "    model_tl.classifier = nn.Linear(num_ftrs, N_LABELS)\n",
        "    # model_tl.classifier = nn.Sequential(nn.Linear(num_ftrs, N_LABELS), \n",
        "    #                                     nn.Sigmoid())\n",
        "    model_tl.classifier = model_tl.classifier.cuda()\n",
        "    # If multiple-class classifier were used, a Sequential\n",
        "    # container would be necessary. \n",
        "    # E.g., nn.Sequential(nn.Linear(num_ftrs, N_LABELS), nn.Softmax())\n",
        "    \n",
        "    # Define Loss Function (Binary Cross-Entropy Loss)\n",
        "    # criterion = nn.BCELoss()\n",
        "    # criterion = nn.BCEWithLogitsLoss() # It applies a sigmoid activation internally\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Define optimizer for the new model\n",
        "    # With Adam Optimizer\n",
        "    optimizer = optim.Adam(model_tl.parameters(), lr=LR, betas=(0.9, 0.999))\n",
        "    # With SGD Optimizer\n",
        "    # Observe that all parameters are being optimized\n",
        "    # optimizer = optim.SGD(model_tl.parameters(), lr=0.001, momentum=0.9)\n",
        "    \n",
        "    # Decay LR by a factor of 0.1 every 7 epochs (when using SGD optimizer)\n",
        "    # exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "    # Train COVID model\n",
        "    model, best_epoch = train_model(model_tl, criterion, optimizer, LR, num_epochs=NUM_EPOCHS,\n",
        "                                    dataloaders=dataloaders, dataset_sizes=dataset_sizes, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    # get preds and AUCs on test fold\n",
        "    # preds, aucs = E.make_pred_multilabel(\n",
        "    #     data_transforms, model, PATH_TO_IMAGES)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    binary_classifier = True\n",
        "    if binary_classifier:\n",
        "        PATH_TO_IMAGES = \"../images/cleaned_up/binary_classifier\"\n",
        "    else:\n",
        "        PATH_TO_IMAGES = \"../images/cleaned_up/multiclass_classifier\"\n",
        "    \n",
        "    checkpoint_type = 'full_model'\n",
        "    CHEXNET_CHECKPOINT = '../pretrained_chexnet/checkpoint'\n",
        "    FINE_TUNING = False\n",
        "    # Hyperparams for Adam Optimizer: LR=0.001, betas=(0.9, 0.999)\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    best_model = perform_tl_cnn(PATH_TO_IMAGES, CHEXNET_CHECKPOINT, checkpoint_type, \n",
        "                                LEARNING_RATE, WEIGHT_DECAY, FINE_TUNING)\n",
        "    # print(best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}