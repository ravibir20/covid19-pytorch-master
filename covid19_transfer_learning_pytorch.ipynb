{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid19_transfer_learning-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhorapb/covid19-pytorch/blob/master/covid19_transfer_learning_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uog5XjZ-5ZSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46zJ_U9zbgtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Model for transfer learning from CheXNet by: \n",
        "- Pretraining (Feature Extraction): training only \n",
        "the output layer (last fully-connected one).\n",
        "and freezing the remaining ones.\n",
        "- Fine Tuning: training only the classification layer \n",
        "and some of the top convolution layers.\n",
        "\"\"\"\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import (\n",
        "    datasets, \n",
        "    models, \n",
        "    transforms, \n",
        "    utils\n",
        ")\n",
        "\n",
        "# Image imports\n",
        "# from skimage import io, transform\n",
        "# from PIL import Image\n",
        "\n",
        "# General imports\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from shutil import copyfile\n",
        "from shutil import rmtree\n",
        "from pathlib import Path\n",
        "\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# import covid_dataset as COVID_XR\n",
        "# import eval_model as E\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MaaDAgV6Uvy",
        "colab_type": "code",
        "outputId": "0cfce8c9-f17f-4a29-98ce-131757a70601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "root_path = '/drive/My Drive/1-COVID-19_DeepLearning/'\n",
        "# os.chdir(root_path)\n",
        "%cd drive/My\\ Drive/1-COVID-19_DeepLearning/\n",
        "!ls\n",
        "# !git clone https://github.com/jhorapb/covid19-pytorch.git\n",
        "# print(os.listdir())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/1-COVID-19_DeepLearning\n",
            "covid19-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "splS-Rvmru-g",
        "colab_type": "code",
        "outputId": "88705e2d-ec1e-4618-ce54-e8eef2afe837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd covid19-pytorch/covid_models/\n",
        "# !git pull"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/1-COVID-19_DeepLearning/covid19-pytorch/covid_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqO0A1A09OKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "RESULTS_PATH = '../results/fine_tuning_jtest/'\n",
        "use_gpu = torch.cuda.is_available()\n",
        "# device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "# print(\"Device: \" + str(device))\n",
        "gpu_count = torch.cuda.device_count()\n",
        "print(\"Available GPU count: \" + str(gpu_count))\n",
        "\n",
        "def create_bare_densenet(fine_tuning=False, unfrozen_names=None):\n",
        "    PRETRAINED = True\n",
        "    print('ImageNet weights:', PRETRAINED)\n",
        "    bas_densenet_model = models.densenet121(pretrained=PRETRAINED)\n",
        "    for parameter in bas_densenet_model.parameters():\n",
        "          parameter.requires_grad = False\n",
        "\n",
        "    # If Fine Tuning, we unfreeze 'unfrozen_names' layers \n",
        "    # (with more processing resources we could unfreeze the whole network)\n",
        "    if fine_tuning:\n",
        "        # If a list with the name of the layers to be enabled is received\n",
        "        if unfrozen_names:\n",
        "          for name, child in bas_densenet_model.named_children():\n",
        "              if name == 'features':\n",
        "                  for sub_name, sub_child in child.named_children():\n",
        "                      if sub_name in unfrozen_names:\n",
        "                          print('=>', sub_name, 'is being unfrozen.')\n",
        "                          for parameter in sub_child.parameters():\n",
        "                              parameter.requires_grad = True\n",
        "\n",
        "\n",
        "    return bas_densenet_model\n",
        "\n",
        "def load_checkpoint(PATH_CHECKPOINT, mode='state_dict', fine_tuning=False, \n",
        "                    unfrozen_names=None):\n",
        "    #####\n",
        "    # The pre-trained model checkpoint from 'reproduce-chexnet' contains:\n",
        "    # state = {\n",
        "    #     'model': model,\n",
        "    #     'best_loss': best_loss,\n",
        "    #     'epoch': epoch,\n",
        "    #     'rng_state': torch.get_rng_state(),\n",
        "    #     'LR': LR\n",
        "    # }\n",
        "    #####\n",
        "    \n",
        "    # Define new base model\n",
        "    model_tl = models.densenet121(pretrained=False)\n",
        "    model_dict = model_tl.state_dict()\n",
        "    \n",
        "    # Locate checkpoint\n",
        "    chexnet_checkpoint = torch.load(PATH_CHECKPOINT)\n",
        "    if mode == 'state_dict':\n",
        "        # Load pretrained CheXNet model (mode state_dict)\n",
        "        state_dict_chexnet = chexnet_checkpoint['state_dict']\n",
        "        # model_tl = torch.nn.DataParallel(model_tl)\n",
        "    else:\n",
        "        # Load pretrained CheXNet model (mode full_model)\n",
        "        chexnet_model = chexnet_checkpoint['model']\n",
        "        state_dict_chexnet = chexnet_model.state_dict()\n",
        "    \n",
        "    # 1. Filter out unnecessary keys\n",
        "    # state_dict_chexnet = {k: v for k, v in state_dict_chexnet.items() \n",
        "    #                       if k in model_dict}\n",
        "\n",
        "    # NEW for Correcting keys:\n",
        "    import re\n",
        "\n",
        "    #--------> TRY 2: Code modified from torchvision densenet source for loading from pre .4 densenet weights.\n",
        "    \n",
        "    remove_data_parallel = False # Change if you don't want to use nn.DataParallel(model)\n",
        "\n",
        "    pattern = re.compile(\n",
        "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "    for key in list(state_dict_chexnet.keys()):\n",
        "        match = pattern.match(key)\n",
        "        new_key = match.group(1) + match.group(2) if match else key\n",
        "        new_key = new_key[7:] if remove_data_parallel else new_key\n",
        "        state_dict_chexnet[new_key] = state_dict_chexnet[key]\n",
        "        # Delete old key only if modified.\n",
        "        if match:\n",
        "            del state_dict_chexnet[key]\n",
        "\n",
        "    #-----------> end try 2\n",
        "\n",
        "    # 2. Load the new state dict\n",
        "    model_tl.load_state_dict(state_dict_chexnet, strict=False)\n",
        "    \n",
        "    # epoch = chexnet_checkpoint['epoch']\n",
        "    # loss = chexnet_checkpoint['loss']\n",
        "    # LR = chexnet_checkpoint['LR']\n",
        "    # print('\\nPretrained model:', model_tl)\n",
        "    \n",
        "    # Freeze the parameters for feature extraction (ony for pretraining), \n",
        "    # NO Fine Tuning\n",
        "    for parameter in model_tl.parameters():\n",
        "        parameter.requires_grad = False\n",
        "    # If Fine Tuning, we unfreeze 'unfrozen_names' layers \n",
        "    # (with more processing resources we could unfreeze the whole network)\n",
        "    if fine_tuning:\n",
        "        # If a list with the name of the layers to be enabled is received\n",
        "        if unfrozen_names:\n",
        "          for name, child in model_tl.named_children():\n",
        "              if name == 'features':\n",
        "                  for sub_name, sub_child in child.named_children():\n",
        "                      if sub_name in unfrozen_names:\n",
        "                          print('=>', sub_name, 'is being unfrozen.')\n",
        "                          for parameter in sub_child.parameters():\n",
        "                              parameter.requires_grad = True\n",
        "    \n",
        "    del chexnet_checkpoint\n",
        "    return model_tl\n",
        "\n",
        "\n",
        "def save_checkpoint(model, best_loss, best_accuracy, epoch, LR, optimizer,\n",
        "                    filename):\n",
        "    \"\"\"\n",
        "    Saves checkpoint of torchvision model during training.\n",
        "\n",
        "    Args:\n",
        "        model: torchvision model to be saved\n",
        "        best_loss: best val loss achieved so far in training\n",
        "        epoch: current epoch of training\n",
        "        LR: current learning rate in training\n",
        "        optimizer: pytorch optimizer to be saved\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    state = {\n",
        "        'model': model.state_dict(),\n",
        "        'best_accuracy': best_accuracy,\n",
        "        'best_loss': best_loss,\n",
        "        'epoch': epoch,\n",
        "        'rng_state': torch.get_rng_state(),\n",
        "        'LR': LR,\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    torch.save(state, RESULTS_PATH + filename)\n",
        "\n",
        "def show_optimizer_params(optimizer_state_dict):\n",
        "    # Print optimizer's state_dict\n",
        "    # print(\"\\nOptimizer's state_dict:\")\n",
        "    for var_name in optimizer_state_dict:\n",
        "        print(var_name, '\\t', optimizer_state_dict[var_name])\n",
        "\n",
        "def train_model(\n",
        "        model,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        LR,\n",
        "        num_epochs,\n",
        "        dataloaders,\n",
        "        dataset_sizes,\n",
        "        weight_decay):\n",
        "    \"\"\"\n",
        "    Fine tunes torchvision model to COVID-19 CXR data.\n",
        "\n",
        "    Args:\n",
        "        model: torchvision model to be finetuned (densenet-121 in this case)\n",
        "        criterion: loss criterion (binary cross entropy loss, BCELoss)\n",
        "        optimizer: optimizer to use in training (SGD)\n",
        "        LR: learning rate\n",
        "        num_epochs: continue training up to this many epochs\n",
        "        dataloaders: pytorch train and val dataloaders\n",
        "        dataset_sizes: length of train and val datasets\n",
        "        weight_decay: weight decay parameter we use in SGD with momentum\n",
        "    Returns:\n",
        "        model: trained torchvision model\n",
        "        best_epoch: epoch on which best model val loss was obtained\n",
        "\n",
        "    \"\"\"\n",
        "    since = time.time()\n",
        "\n",
        "    start_epoch = 1\n",
        "    best_loss = 999999\n",
        "    best_acc = 0.0\n",
        "    best_epoch = -1\n",
        "    last_train_loss = -1\n",
        "\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 17)\n",
        "\n",
        "        # set model to train or eval mode based on whether we are in train or\n",
        "        # val; necessary to get correct predictions given batchnorm\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                # model.train(True)\n",
        "                model.train(True)\n",
        "            else:\n",
        "                # model.train(False)\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "            total_done = 0\n",
        "            \n",
        "            # Iterate over dataset (train/val)\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                batch_size = inputs.shape[0]\n",
        "                inputs = Variable(inputs.cuda())\n",
        "                labels = Variable(labels.cuda()) #.long()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):    \n",
        "                    # Compute loss\n",
        "                    outputs = model(inputs)\n",
        "                    something_else, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                \n",
        "                # Backward pass: compute gradient and update \n",
        "                # parameters in training \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Computing loss for the whole batch dataset\n",
        "                running_loss += loss.item() * batch_size\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            \n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                last_train_loss = epoch_loss\n",
        "\n",
        "            print('{} epoch {}=> Loss: {:.4f} | Acc: {:.4f} | Data size: {}'.format(\n",
        "                phase, epoch, epoch_loss, epoch_acc, dataset_sizes[phase]))\n",
        "\n",
        "            if phase == 'val':\n",
        "                # # Decay learning rate if validation loss plateaus in this epoch\n",
        "                # if epoch_loss > best_loss:\n",
        "                #     decayed_LR = LR / 10\n",
        "                #     print('Decay Loss from {} to {} \\\n",
        "                #             as not seeing improvement in val loss'.format(\n",
        "                #                 str(LR), str(decayed_LR))\n",
        "                #             )\n",
        "                    # LR = decayed_LR\n",
        "                    # # Create new optimizer with lower learning rate\n",
        "                    # optimizer = optim.Adam(\n",
        "                    #     filter(\n",
        "                    #         lambda p: p.requires_grad, \n",
        "                    #         model_tl.parameters()), \n",
        "                    #     lr=LR, betas=(0.9, 0.999))\n",
        "                #     print(\"Created new optimizer with LR \" + str(LR))\n",
        "                \n",
        "                # Checkpoint model if has best val loss yet\n",
        "                if epoch_loss < best_loss:\n",
        "                    best_loss = epoch_loss\n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_epoch = epoch\n",
        "                    save_checkpoint(model, best_loss, best_acc, \n",
        "                                    epoch, LR, optimizer, \n",
        "                                    'temp_tl_finetuning_checkpoint')\n",
        "\n",
        "                # Log training and validation loss over each epoch\n",
        "                with open(RESULTS_PATH + '/log_train', 'a') as logfile:\n",
        "                    logwriter = csv.writer(logfile, delimiter=',')\n",
        "                    if(epoch == 1):\n",
        "                        logwriter.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
        "                    logwriter.writerow([epoch, last_train_loss, epoch_loss])\n",
        "\n",
        "        total_done += batch_size\n",
        "        if(total_done % (100 * batch_size) == 0):\n",
        "            print(\"completed \" + str(total_done) + \" so far in epoch\")\n",
        "\n",
        "        # Apply early stopping if there is no val loss improvement in 3 epochs\n",
        "        if ((epoch - best_epoch) >= 10):\n",
        "            print(\"No improvement in the model accuracy in 3 epochs, stop!\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best Validation Loss: {:4f}'.format(best_loss))\n",
        "    print('Best Validation Accuracy: {:4f}'.format(best_acc))\n",
        "    print('Best Epoch: {}'.format(best_epoch))\n",
        "\n",
        "    # Load best model weights to return\n",
        "    checkpoint_best = torch.load(RESULTS_PATH + 'temp_tl_finetuning_checkpoint')\n",
        "    model_state_dict = checkpoint_best['model']\n",
        "    model.load_state_dict(model_state_dict)\n",
        "\n",
        "    best_filename = 'best_tl_finetuning_checkpoint'\n",
        "    if os.path.exists(RESULTS_PATH + best_filename):\n",
        "        checkpoint_best_ever = torch.load(RESULTS_PATH + best_filename)\n",
        "        temp_best_acc_check = checkpoint_best['best_accuracy']\n",
        "        ever_best_acc_check = checkpoint_best_ever['best_accuracy']\n",
        "        if temp_best_acc_check > ever_best_acc_check:\n",
        "            print('A new best ever model was found and will replace the \\\n",
        "            previous one! New: {:4f} | Old: {:4f}'.format(\n",
        "                temp_best_acc_check, ever_best_acc_check\n",
        "            ))\n",
        "            save_checkpoint(model, best_loss, best_acc, \n",
        "                            epoch, LR, optimizer, \n",
        "                            'best_tl_finetuning_checkpoint')\n",
        "    else:\n",
        "        print('A new best ever model will be created!')\n",
        "        save_checkpoint(model, best_loss, best_acc, \n",
        "                              epoch, LR, optimizer, \n",
        "                              'best_tl_finetuning_checkpoint')\n",
        "\n",
        "    return model, best_epoch, checkpoint_best\n",
        "\n",
        "\n",
        "def perform_tl_cnn(PATH_TO_IMAGES, CHEXNET_CHECKPOINT, checkpoint_type, \n",
        "                   LR, WEIGHT_DECAY, FINE_TUNING, unfrozen_names, \n",
        "                   USE_CHECKPOINT=False):\n",
        "    \"\"\"\n",
        "    Trains model to COVID-19 dataset.\n",
        "\n",
        "    Args:\n",
        "        PATH_TO_IMAGES: path to COVID-19 image data collection\n",
        "        LR: learning rate\n",
        "        WEIGHT_DECAY: weight decay parameter for SGD\n",
        "\n",
        "    Returns:\n",
        "        preds: torchvision model predictions on test fold with ground truth for comparison\n",
        "        aucs: AUCs for each train,test tuple\n",
        "    \"\"\"\n",
        "    NUM_EPOCHS = 20\n",
        "    # Since the COVID-19 dataset at the moment is considerably small, \n",
        "    # it makes sense to use Batch Gradient Descent (all the samples \n",
        "    # being used to update the model parameters)\n",
        "    minibatch_gd = False\n",
        "    BATCH_SIZE = 375 if not minibatch_gd else 10\n",
        "\n",
        "    # Create path to save model results\n",
        "    Path(RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ImageNet parameters for normalization\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # Binary classifier\n",
        "    # N_LABELS = 1\n",
        "    # Multi-class classifier\n",
        "    N_LABELS = 2\n",
        "\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(means, stds)\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(means, stds)\n",
        "        ]),\n",
        "    }\n",
        "    \n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(PATH_TO_IMAGES, x), \n",
        "                                              data_transforms[x]) \n",
        "                      for x in ['train', 'val']}\n",
        "    \n",
        "    # Option num. workers 8\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, \n",
        "                                                  shuffle=True, num_workers=1) \n",
        "                   for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "    print('CLASS NAMES\\n:', class_names, '\\n')\n",
        "    \n",
        "    if USE_CHECKPOINT:\n",
        "      print('Loading pre-trained CheXNet...')\n",
        "      print('Fine-tuning:', FINE_TUNING)\n",
        "      # Load pre-trained CheXNet model\n",
        "      model_tl = load_checkpoint(CHEXNET_CHECKPOINT, mode=checkpoint_type, \n",
        "                                fine_tuning=FINE_TUNING, \n",
        "                                unfrozen_names=unfrozen_names)\n",
        "    else:\n",
        "      print('Initializing bare DenseNet with ImageNet weights...')\n",
        "      # Initialize bare DenseNet 121 trained over ImageNet\n",
        "      model_tl = create_bare_densenet(fine_tuning=FINE_TUNING, \n",
        "                                      unfrozen_names=unfrozen_names)\n",
        "\n",
        "    # Verify if GPU is available\n",
        "    if not use_gpu:\n",
        "        raise ValueError(\"GPU is required\")\n",
        "    if use_gpu:\n",
        "        model_tl = model_tl.cuda()\n",
        "\n",
        "    # print('Pre-trained Model:\\n', model_tl)\n",
        "    num_ftrs = model_tl.classifier.in_features\n",
        "    # Size of each output sample.\n",
        "    model_tl.classifier = nn.Linear(num_ftrs, N_LABELS)\n",
        "    # model_tl.classifier = nn.Sequential(nn.Linear(num_ftrs, N_LABELS), \n",
        "    #                                     nn.Sigmoid())\n",
        "    model_tl.classifier = model_tl.classifier.cuda()\n",
        "    # print('COVID Model:\\n', model_tl)\n",
        "    # If multiple-class classifier were used, a Sequential\n",
        "    # container would be necessary. \n",
        "    # E.g., nn.Sequential(nn.Linear(num_ftrs, N_LABELS), nn.Softmax())\n",
        "\n",
        "    # Define Loss Function (Binary Cross-Entropy Loss)\n",
        "    # criterion = nn.BCELoss()\n",
        "    # criterion = nn.BCEWithLogitsLoss() # It applies a sigmoid activation internally\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Define optimizer for the new model\n",
        "    # With Adam Optimizer\n",
        "    optimizer = optim.Adam(filter(lambda p:p.requires_grad, model_tl.parameters()), \n",
        "                           lr=LR, betas=(0.9, 0.999))\n",
        "    # With SGD Optimizer\n",
        "    # Observe that all parameters are being optimized\n",
        "    # optimizer = optim.SGD(model_tl.parameters(), lr=0.001, momentum=0.9)\n",
        "    \n",
        "    # Decay LR by a factor of 0.1 every 7 epochs (when using SGD optimizer).\n",
        "    # ---> If we use this, then we should not perform Decay LR in the training \n",
        "    # function, since we are defining a schedule already in that case.\n",
        "    # exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "    # Train COVID model\n",
        "    model, best_epoch, checkpoint_best = train_model(\n",
        "        model_tl, criterion, optimizer, LR, num_epochs=NUM_EPOCHS, \n",
        "        dataloaders=dataloaders, dataset_sizes=dataset_sizes, weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "    print('Best Epoch Saved: ', checkpoint_best['epoch'])\n",
        "\n",
        "    # Print best optimizer saved params\n",
        "    # show_optimizer_params(checkpoint_best['optimizer'])\n",
        "    # get preds and AUCs on test fold\n",
        "    # preds, aucs = E.make_pred_multilabel(\n",
        "    #     data_transforms, model, PATH_TO_IMAGES)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    binary_classifier = True\n",
        "    if binary_classifier:\n",
        "        PATH_TO_IMAGES = \"../images/cleaned_up/binary_classifier\"\n",
        "    else:\n",
        "        PATH_TO_IMAGES = \"../images/cleaned_up/multiclass_classifier\"\n",
        "    \n",
        "    #### Model configuration\n",
        "    checkpoint_type = 'full_model'\n",
        "    CHEXNET_CHECKPOINT = '../pretrained_chexnet/checkpoint'\n",
        "    USE_CHECKPOINT = True\n",
        "    FINE_TUNING = True\n",
        "    #### End of Model configuration\n",
        "\n",
        "    if FINE_TUNING:\n",
        "        # There are four main denseblocks: 'denseblock1', 'denseblock2', \n",
        "        # 'denseblock3', 'denseblock4'\n",
        "        unfrozen_names = ['denseblock4']\n",
        "    else:\n",
        "        unfrozen_names = None\n",
        "    # Hyperparams for Adam Optimizer: LR=0.001, betas=(0.9, 0.999)\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    best_model = perform_tl_cnn(PATH_TO_IMAGES, CHEXNET_CHECKPOINT, \n",
        "                                checkpoint_type, LEARNING_RATE, WEIGHT_DECAY, \n",
        "                                FINE_TUNING, unfrozen_names, \n",
        "                                USE_CHECKPOINT=USE_CHECKPOINT)\n",
        "    # print(best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}